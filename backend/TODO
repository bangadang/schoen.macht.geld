# Sch√∂n-Macht-Geld Backend TODO

## Stock Endpoints

* [x] Don't return whole price history
* [x] Add random and limit parameters to the stock endpoint to select N random stocks
* [x] Modify stock admin form to support camera capture
* [x] Define "last price" and compute percentage change (reference_price + percentage_change)
* [x] Create computed price history (StockSnapshot with periodic scheduler job)
* [x] Create stock ranking (including places gained/lost since last update) when creating snapshot.
* [ ] Check if PK columns of PriceEvent and StockSnapshot are required or if we can use TICKER + timestamp
* [x] Rework swipe endpoint
  * [x] Inputs/params
  * [x] Return type
  * [x] Make price increase/decrease more random
    * [x] Randomness
    * [x] Current price
    * [x] Up/down streak
    * [x] Swipes of user during the last few minutes
* [ ] Live feed (wait for frontend rewrite from Firestore to new backend)
* [ ] Search/filter stocks endpoint (low priority)
* [x] Make limit_price_events sync (was async but didn't need to be)
* [x] Validate if current file size limit is enough & check if image type enumeration is complete
* [x] Compress uploaded images if too large (or just in general recompress/resize all uploaded images)

## Polishing

* [ ] Complete this file/at least this & the "Must have use-cases" section"
* [ ] Setup
  * [x] HTTP server for images & static assets (Caddy documented in README)
  * [ ] DB setup (postgres overkill?)
  * [x] Create docker compose for the backend (and possibly frontend?)
  * [x] Run script for backend (`python -m app` documented in README)
  * [x] Automate start of all required services & stuff so people other than me can do it (docker compose with restart: unless-stopped)
  * [ ] Document setup, configuration & troubleshooting
* [x] Backups/Recovery
  * [x] Docker container creates atomic SQLite snapshots + rsync images every 10 min
  * [x] Backups written to ./backups/ (can be different drive/mount)
  * [x] MacBook pull script documented for disaster recovery
* [ ] Benchmark DB
* [x] Make sure that server restarts when it crashes (docker compose restart: unless-stopped)
* [ ] Logging & Monitoring
  * [ ] Structured logging for production
  * [ ] Error alerting (crashes, exceptions)
* [x] Production Config
  * [ ] Environment-specific settings (dev/prod) - not needed for party
  * [x] CORS origins for production
  * [x] SSL/TLS setup (via Caddy reverse proxy)
* [x] Database
  * [x] Migration strategy (alembic configured with async support)
* [x] Deployment
  * [x] Health check endpoint improvements (DB connectivity, scheduler status, disk space)
  * [x] Graceful shutdown handling (scheduler.shutdown with wait=True)

### Must have use-cases

* Register a new stock (via admin panel)
  * Enter ticker manually
  * Enter title
  * Upload photo
  * Set initial price
  * AI-generated description (TBD)
* Display market map
* Display rankings
* Display news ticker
* Display the title chart
  * Initial stock
  * Periodically switch to different stock
* Tinder
  * Load initial stock batch (empty DB)
  * Load next batch of stocks to swipe
  * Swipe left/right on a stock (and therefore change the stocks price)
* Administration
  * List all stocks
  * Edit a stock
    * Title
    * Description (regenerate or manually edit?)
    * Photo
    * Set price
    * Reset stock (clear price history, reset to initial price)
  * Remove a stock
  * Reset game state (clear all price events, snapshots, reset all stocks)
  * View swipe statistics (for debugging/tuning)
  * AI task management (lowest priority)
    * View/manage AI generation tasks
    * Apply generated content to stocks

## AI

### Spending Tracking

* [ ] Display remaining AI credits in webapp

### News

* [ ] Generate news updates that react to stock price changes, overtakes, profits etc.

### Logo

* [ ] Generate a logo for a stock using its title & description
* [ ] Improve logo prompt

## Bugs
* [x] Random image name generation leads to 404

## Ideas

* [ ] Win/loss for right/let swipe depends on price, randomness (maybe swipe history?)
* [ ] Initial price is defined by game or something interactive
* [ ] Random stock movement
* [ ] Simulate real life stock market events like financial crisis, crash, extreme swings where even right swiping loses money and other situations with the opposite
  * Could be admin-triggered during the party (e.g., "MARKET CRASH" button)
  * Or scheduled/random events with announcements on the news ticker
* [ ] Export/Import stocks (CSV, JSON) for backup or reuse between parties

## Open Questions

* Where does the backend run during the party?
* How do the displays access the backend?
* How do the people registering people/administrating the stock exchange access the backend?
* What's the network setup at the venue? (WiFi? Ethernet? Offline fallback?)
* How many concurrent users/swipes expected at peak?
* Multiple kiosks - do they need to coordinate/sync?
* Is any initial seed data required? If so who provides it and how should we import it? AI Gen?
* Game timing: market open/close times? Rounds?
* What happens if server crashes mid-party? Recovery plan?
* Who has admin access during the party?

---

## Code Review Findings (2024-12-20)

### Quick Wins

* [x] Fix broken debug log in storage.py:136
  - Log message format expects 4 values ("Resized image from {}x{} to {}x{}")
  - But passes: content[:10] (bytes!), img.width, img.height (only 3 values, first is wrong type)
  - Either fix the format string or remove this misleading log

* [x] Standardize logging to use loguru format strings
  - Many places used f-strings: `logger.info(f"Created task {task.id}")`
  - Should use lazy formatting: `logger.info("Created task {}", task.id)`
  - Files to fix: ai.py (lines 93, 132, 173, etc.), scheduler.py (lines 176, 182, etc.)
  - Lazy formatting is more performant (string not built if log level disabled)

### Anti-Patterns

* [x] Fix N+1 query in snapshot cleanup (scheduler.py)
  - Was: loops through each ticker, individual DELETEs
  - Now: bulk DELETE with notin_() query, single commit

* [x] Fix class-level _old_image state in admin.py
  - Was: CLASS variable causing race conditions
  - Now: uses request.state for per-request storage

* [x] Replace broad 'except Exception' in storage.py
  - Now catches: UnidentifiedImageError, OSError, ValueError

* [x] Replace broad 'except Exception' in scheduler.py
  - Now catches: AtlasCloudError, AtlasCloudTransientError, OSError

* [ ] Refactor module-level singletons for testability (OPTIONAL - low priority for party)
  - storage.py:43 - `storage = NonOverwritingFileSystemStorage(path=settings.static_dir)`
  - atlascloud.py:134 - `atlascloud = AtlasCloudClient()`
  - These are instantiated at import time, reading settings immediately
  - Makes testing harder (can't inject mock settings)
  - Fix: use FastAPI dependency injection or lazy initialization

### Inconsistencies

* [x] Fix SwipeToken to use SwipeDirection enum instead of raw strings
  - Schema defines: `class SwipeDirection(str, Enum): LEFT = "left"; RIGHT = "right"`
  - swipe_token.py now accepts SwipeDirection enum throughout
  - SwipeStats.streak_direction is now SwipeDirection | None

* [x] Standardize API response types (Pydantic models vs dicts)
  - Created `MessageResponse` schema in schemas/ai.py
  - Updated apply_result and delete_task to use MessageResponse
  - All endpoints now return proper Pydantic models

### Resilience (HIGH PRIORITY for party)

* [x] Add graceful shutdown handling
  - scheduler.py now uses `scheduler.shutdown(wait=True)`
  - Waits for in-progress jobs to complete before shutdown

* [x] Add circuit breaker for AtlasCloud API calls
  - Opens after 5 consecutive failures, stays open for 60s
  - Fast-fails with AtlasCloudError when circuit is open
  - Implemented in atlascloud.py with CircuitBreaker class

* [x] Add retry logic for transient AtlasCloud failures
  - Uses `tenacity` with 3 attempts and exponential backoff (1-10s)
  - Retries on: 5xx errors, timeouts, connection errors
  - Does NOT retry on: 4xx client errors (bad request, auth, etc.)

### Monitoring

* [x] Expand health check to verify DB connectivity
  - Now checks: database (SELECT 1), scheduler (running + job count), disk (free GB)
  - Returns "ok" or "degraded" status with detailed checks object

* [ ] Add structured logging (JSON format) for observability (OPTIONAL)
  - Current logs are human-readable but hard to aggregate/search
  - Loguru supports JSON output: `logger.add(sink, serialize=True)`
  - Useful for debugging issues during/after the party

* [ ] Add metrics endpoint (Prometheus-compatible) (OPTIONAL)
  - Track: stocks count, swipes/min, AI tasks queue depth, response times
  - Library: `prometheus-fastapi-instrumentator`
  - Nice to have for monitoring during party

### Data Issues

* [x] Fix ticker collision handling
  - Was: auto-generated ticker from title could collide
  - Now: ticker is a required field provided by user
  - Returns 409 Conflict if ticker already exists
  - Added validation: 1-10 alphanumeric chars, uppercase normalized

* [x] Fix race condition on concurrent swipes
  - Two swipes on same stock at same time could both read same price
  - Both calculate delta from that price, one update gets lost
  - Fixed: per-ticker asyncio locks serialize swipes on the same stock
  - Different stocks can still be swiped concurrently (no global lock)

### Documentation (LOW PRIORITY)

* [ ] Fix README: Python version says 3.14+ but 3.14 doesn't exist yet
  - Should be 3.12+ or whatever version is actually required
  - Also update .python-version and pyproject.toml if needed

* [ ] Fix README: Model name discrepancy
  - README says: google/gemini-3-flash-preview-developer
  - config.py says: google/gemini-3-flash-preview
  - Sync these to match actual working model

* [ ] Document error response format & codes in README
  - What HTTP codes are returned for what errors?
  - What does the error response body look like?
  - Validation error format from Pydantic?

* [ ] Add architecture overview / data flow diagram to README
  - How scheduler ticks prices and creates snapshots
  - How swipe tokens work (stateless client tracking)
  - How AI tasks are queued and processed

* [ ] Document LOGURU_LEVEL env var in README
  - Present in .env.example but not in Configuration table
